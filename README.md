# Regional Sales Analysis Project
[![My Skills](https://skillicons.dev/icons?i=py,vscode,mysql)](https://skillicons.dev) ![My Skills](https://go-skill-icons.vercel.app/api/icons?i=pbi&titles=true)

---

## Executive Summary

This project simulates a real-world regional sales analysis scenario. It demonstrates my ability to:

- Clean and transform raw data using Python
- Structure and query data in MySQL
- Extract actionable insights through exploratory analysis
- Build a stakeholder-ready Power BI dashboard

**Goal:** Uncover performance trends, identify inefficiencies, and recommend data-driven strategies to improve regional sales outcomes.

---

## Table of Contents

- [üìä Business Context](#-business-context)
- [üõ†Ô∏è Technical Workflow](#-technical-workflow)
- [üìå Key Deliverables](#-key-deliverables)
- [üì∏ Dashboard Preview](#-dashboard-preview)
- [üìö Tools Used](#-tools-used)

---

## Business Context
### Problem Statement
Despite strong national sales figures, the company's sales team often lacks a clear, data-driven understanding of how performance varies across U.S. regions. This gap in visibility makes it difficult to identify growth opportunities, allocate resources effectively, and respond to market shifts.

### Business Challenges
- Revenue and profitability fluctuate inconsistently across regions, with no centralized view of performance drivers.

- Seasonal trends, top-selling SKUs, and channel-level profitability are obscured by fragmented reporting.

- Strategic decisions are often reactive rather than proactive due to limited historical insight.

### Project Goal
Leverage 5 years of historical sales data to:
- Uncover regional trends and performance disparities
- Evaluate profitability across products, channels, and seasons
- Pinpoint growth levers and inform resource allocation strategies
---

## Technical Workflow

| Step | Description |
|------|-------------|
| **1. Data Collection** | Consolidated multi-source sales data from Excel files and conducted schema analysis to ensure structural consistency across inputs. |
| **2. Data Cleaning** | Applied robust preprocessing techniques to remove duplicates, impute missing values, standardize formats, and normalize column structures for analytical readiness. |
| **3. Data Loading & Initial Exploration** | Loaded cleaned data into Google Colab and used Python for initial profiling, schema validation, and exploratory checks to guide deeper analysis. |
| **4. Exploratory Data Analysis** | Conducted historical trend analysis to uncover performance patterns, outliers, and correlations. Synthesized findings into clear, stakeholder-ready insights. |
| **5. Dashboarding & Communication** | Built a live, interactive Power BI dashboard tailored for business users, enabling self-service exploration and supporting strategic conversations through visual storytelling. |

---

## Key Deliverables

- ‚úÖ Cleaned and structured dataset with engineered features  
- ‚úÖ SQL queries for slicing data by region, category, and time  
- ‚úÖ Visualizations highlighting trends, outliers, and seasonal patterns  
- ‚úÖ Power BI dashboard with actionable insights and recommendations

---
## Dashboard Preview

---

## Tools Used

| Tool | Purpose | 
|--------------------|----------------------------------|
| Python | Data cleaning, transformation | 
| Pandas | Tabular data manipulation |
| Matplotlib & Seaborn | Visualizations for EDA | 
| MySQL | Data storage and querying | 
| Power BI | Dashboard creation |
